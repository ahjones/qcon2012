* QCon
** Building highly available systems in Erlang
   Joe Armstrong
*** The types of highly available systems
    How do you get ten nines availability? Why even ten? The number is
    arbitrary.

    - Washing machine / pacemaker (!)
      Very specialised, embedded. Processor and the data are in the
      same place, so easy to program.
    - Deep space mission
      The only things that'll be left of humans after we're gone (so far)
    - Aircraft control system
      Wait until the plane is on the ground before changing the
      software. Shut down the nuclear plant before updating the
      software.
    - Internet systems
      This talk. Data and processing separate and distributed.

*** Internet available systems
    Systems like this need highly available data.

    Computation can be performed anywhere.

    We want many routes to the data.

**** Where is my data?
     If there are 10 million computers and my data is on ten of them,
     I can't ask each computer if it has my data.

***** Algorithm: [[http://en.wikipedia.org/wiki/Chord_(peer-to-peer)][Chord]]
      + Hash the computers IPs
      + Sort the hashes
      + Hash the lookup key
      + Put the data on the first machine with a hash that's lower
        than the key's hash

**** How do I store replicas in a consistent way?
     Collect data in parallel. Save data in parallel.
     
     #+BEGIN_QUOTE
     The problem of reliable storage of data has been solved. -- Joe Armstrong
     #+END_QUOTE

*** The six properties of available systems
**** 1. Isolation
     Formal definition: "my program should not fuck up your program". 

**** 2. Concurrency
     Programming in sequential languages is difficult because the
     world isn't seequential.

     "Embarrassingly parallel" problems: web servers.

**** 3. Must be able to detect failure
     If you can't detect it, you can't fix it. This must work across
     machine boundaries. If a machine dies you can't tell if it's the
     machine or the network.

     If you make things synchronous you'll bugger things up.

**** 4. Fault identification
     It's not enough to know that there is a fault.

**** 5. Live code upgrade
     Wow. Why would you want to stop it? We want zero downtime. Early
     requirement for Erlang. At Ericsson you got told off if your
     system was down for more than four minutes in a year. That's ~
     five nines availability.

**** 6. Stable storage
     Suppose all computers crash: you want your data back.

*** Other thoughts
**** Jim Gray
     Fail fast: software should either function correctly or detect
     the fault, signal failure and stop operating.

     If you've got a single process you can't let it die. If you have
     millions of processes, you can let a few thousand die.

     See society: we wander around doing what we want, and if we fall
     down with a heart attack the ambulance rocks up and fixes
     you. Send in the medic.

     Have threads that detect the failure of other threads.

**** Messaging
     #+BEGIN_QUOTE
     The big idea is messaging. -- Alan Kay
     #+END_QUOTE
*** How to satisfy the properties in Erlang
    Use a programming language designed for it. Armstrong can only
    think of one. Ha ha.

    1. Isolation

       Isolate processes so that they can't damage one
       another. No shared memory, lightweight.

       Treating failure with shared memory is very difficult.

    2. Concurrency

       Run the processes in parallel. Hardware design will mean that
       soon we're able to run many processes concurrently.

       Erlang has network transparency so the processes might be
       running elsewhere.

    3. Failure detecting

       Erlang processes can detect failure. This is out of bound: not
       a normal message. It's a signal. It's messy if you handle
       failure in the same place that you handle normal stuff.
       
       Fix the failure somewhere else. What does failing A have to
       send running B so that B can carry on doing the job that A
       didn't manage.

    4. Fault identification

       Special processes that handle errors.

       #+BEGIN_SRC erlang
       receive
           {'EXIT', Pid, Why} ->
               error_log:log_error({erlang:now(), Pid, Why})
       end
       #+END_SRC

    5. Live code upgrade

       In Erlang you can modify code as it runs.

       #+BEGIN_SRC erlang

...
f1(X) ->
    foo:bar(X), % Call the latest version of this module
    bar(X). % Call this version of bar
       
bar(X) ->
    ...
       #+END_SRC

    6. Stable storage
       Use mnesia
       Use third party storage

**** Fault tolerance implies scalability
*** Reading
    [[http://www.sics.se/~joe/thesis/armstrong_thesis_2003.pdf][Armstrong's Phd. thesis]]
** High availability at Heroku
   Mark McGranaghan @mmcgrana

   Lessons learned from PaaS at Heroku

   Everyone was doing the same over and over again: routing, runtime,
   data. Package these three things, and then apps can use them.

   Runs on AWS.

*** What has to be available?
    API, routing, packaging, data, logging, runtime.

    1,000 instances (virtualised servers), 1,000,000 apps.

*** Architecture
    
**** Platform available HA routing
     Simply, a load balancer. If you lose a back end, the balancer
     makes it transparent to the users.

**** Crashes and supervision
     Code crashes, get used to it. Locally, things like upstart work
     fine. On a distributed platform, you need a global view of
     application health. Supervisor detects exit codes, restart the
     app.

     
***** Crashes as the only code path
      Crash is the same path as normal app exit. This enables you to
      handle the failure of an instance, eg if AWS nukes it.

***** Error kernel
      Gets more reliable as you make it smaller.

**** Message passing
     Nodes communicate to a message broker with narrow, versionable
     JSON messages.

     At any given time there are going to be apps that speak the old
     version, and apps that speak the new vesion.

**** See Erlang
     Heroku tries to solve the same problems that Erlang has. It's not
     surpring that there is a similarity in the approaches.

     One broker is a single point of failure. To get around that:

**** Publish one, subscribe many
     If one broker fails, transparrently failover in the client to a
     different broker.
     
**** Graceful degradation
     If you've got distributed services and you can't read from one,
     gracefully degrade.

     If you can't write to a service, persist an 'owe' write. When the
     service comes back on line, persist the owed write. All billing
     writes tickets locally, then asynchronously writes the
     information to the service.

*** Execution
    Everything outside of architecture: culture, organisation, etc.

    Heroku recently had a problem. They had a post-mortem of the
    incident, which took a team about a week. They've noticed that
    most of the causes weren't completely technical. They involved
    people.

    What are the biggest causes of availability failures? Not the
    architecture. Failed deploys (too fast, too slow); bad visibility;
    cascading feedback.

**** Deployment
     Has to be repeatable.

     #+BEGIN_SRC
     bin/ship --component api --vesion 1234
     #+END_SRC

     Some initial pool of deploy servers: .5% of the nodes. Use the
     data coming back from the nodes to determine whether the deploy
     will be successful. When ready, deploy to others over period of
     between minutes and weeks.

**** Incremental rollout
     Heroku had a large change to the way processes work, but they
     managed to roll it out incrementally without users noticing. 

     Feature flag.

     Core orchestration app.

     Ship the code incrementally *then* ship the feature incrementally
     to new users.

**** Availability

***** Graph it.
       Real time visibility. Availability can be thought of as how
       often things go down, and how long the stay down. Keep an eye
       on it.

***** Service level assertions
      Get the computer to keep an eye on the graphs. If the graph
      enters the red state, there's probably a problem.

      #+BEGIN_SRC
assert(p99_latency < 50)
      #+END_SRC

      Time of day isn't accounted for, but mostly that's not a
      problem: they're looking out for catastrophic failure. Perhaps
      they'll consider using the derivative and the second derivative.

***** Flow control and backpressure
      Eventually flow will get to a node that can't handle the traffic
      that reaches. Potentially the whole branch that leads to the
      node can get fried. If you have flow control, you can divert the
      excess traffic away from the sensitive node, and avoid breaking
      the path for everyone. Some traffic will get a 500, but not all.

      echo 0 > /etc/rates/publish

      This'll get picked up by the controller.

** Anomoly detection, fault tolerance, anticipation
   John Allspaw @allspaw

*** Four cornerstones
    - Anticipation
    - Monitoring
    - Response
    - Learning

*** Monitoring and anomoly detection
    Things break. It's harder to find out that you'd think it would
    be.

**** Active health check
     HTTP call to service
     - Pros
       - Easy to implement
       - Easy to understand
       - Well-known pattern
     - Cons
       - Messaging can fail
       - Limited scalability

***** Supervisor sensitivity
      1 sec timeout, 1 retry, 3 sec interval.

      Just because you want to poll something every three seconds
      doesn't mean it's going to happen every three seconds.

      How many seconds of errors can you tolerate serving?

**** Passive health check
     "I'm alive"
     - Pros
       - Efficient
       - Different scalability
       - Fewer moving parts
       - Less exposure
       - Can submit to multiple places
       - Can scale out monitory to a much larger architecture
     - Cons
       - Non ideal for network

**** Passive even logging
     True fire and forget

     - Pros
       - On demand publish
     - Cons
       - Onus is on the app
**** Context
     You've got to understand what's happening at the time. Eg: at
     Christmas you may not have the same behaviour as normal.

     Static thresholds are difficult.

     148,000 metrics at Etsy.

     Finding out what's normal is a big deal. How do you know if a
     drop or a lift is something that you've got to do something
     about?

**** Smoothing
     - Moving average is a possibility
     - [[http://en.wikipedia.org/wiki/Exponential_smoothing][Holt-Winters exponential smoothing]]
       Make a forcast of time series data, the most recent data has an
       exponentially larger influence on the prediction than later
       data.

       Can use it to work out if something is out of bounds. You get a
       Holt-Winters aberration.
